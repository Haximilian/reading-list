<!DOCTYPE html>

<html lang="en">

<head>

    <meta charset="UTF-8" />

    <title>reading-list</title>

    <meta name="viewport" content="width=device-width,initial-scale=1" />

    <meta name="description" content="" />

    <link rel="icon" href="favicon.png">

</head>

<body>

    <h1>reading-list</h1>

    <table border="1" cellspacing="0" cellpadding="5">
        <tr>
            <th>title</th>
            <th>status</th>
            <th>date</th>
            <th>notes</th>
        </tr>
        <tr>
            <td>
                <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model</a>
            </td>
            <td>‚úÖ [completed]</td>
            <td>May 9, 2025</td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>
            </td>
            <td>‚úÖ [completed]</td>
            <td>May 11, 2025</td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a>
            </td>
            <td>‚úÖ [completed]</td>
            <td>May 23, 2025</td>
            <td>
                In word2vec, you learn embedding values such that the dot product of those embeddings corresponds to the likeliness of those words being neighbours. The window size to determine wether a word‚Äôs a neighbour with another word is a hyper-parameter. The paper also uses ‚ÄúNoise Contrastive Estimation‚Äù to create negative examples. Otherwise, the training dataset would only contain positive example. The article didn‚Äôt state this explicitly but neural networks chain logistic regression; each ‚Äúneuron‚Äù, node or perceptron in a neural network corresponds to a logistic regression function which equals sigmoid(Bx). If the first hidden layer contains 3 nodes, we can represent the activations of that layer as [LogisticRegression(B0, x), LogisticRegression(B1, x), LogisticRegression(B2, x)] where x represents the input to the network.
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://arxiv.org/pdf/1301.3781">Efficient Estimation of Word Representations in Vector Space</a>
            </td>
            <td>‚úÖ [completed]</td>
            <td>May 24, 2025</td>
            <td>
                They didn‚Äôt state this explicitly but, they would like to predict the next word in a sentence. NNLM, the previous state of the art transforms words into 1-of-V vectors. V refers to the size of the vocabulary. Using a common projection matrix with V * D parameters, the algorithm projects context size N = 10 words to the project layer with N * D values. The hidden layer contains H activations and afterwards you transform that into an 1-of-V vector again which you would apply a softmax. In total, this architecture has V * D + N * D * H + H * V ‚Äúcomputational complexity per training cycle‚Äù.

                A log-linear model is a mathematical model that takes the form of a function whose logarithm equals a linear combination of the parameters of the model.

                The term back-propagation and gradient descent are not the same. The term back-propagation refers to the calculation of the gradient while gradient descent refers to the minimization of the loss function by adjusting the weights according to the gradient.

                Why do they call an embedding a ‚Äúdistributed representation of words‚Äù? Why do you use cosine distance? Why does the paper state the ‚Äúcomputational complexity per training cycle‚Äù of NNLM as N * D + N * D * H + H * V?
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>
            </td>
            <td>‚è≥ [partial]</td>
            <td>May 12, 2025; May 13 2025</td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://colah.github.io/posts/2015-09-Visual-Information/">Visual Information Theory</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43442.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>
            </td>
            <td>‚è≥ [partial]</td>
            <td>May 12, 2025</td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://arxiv.org/pdf/1607.06450">Layer Normalization</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://eoinmurray.info/boltzmann-machine">Boltzmann Machines</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://www.nist.gov/blogs/cybersecurity-insights/differential-privacy-privacy-preserving-data-analysis-introduction-our">Differential Privacy for Privacy-Preserving Data Analysis: An Introduction to our Blog Series</a>
            </td>
            <td>‚úÖ [completed]</td>
            <td>May 21, 2025</td>
            <td>
                The article states the definition of Differential Privacy.
                Given a dataset A and query Q, you should generate response R with P.a. probability.
                If you're given a dataset B such that B differs with A in only 1 records, for query Q,
                you should generate response R with P.b. probability.

                The ratio of P.a. and P.b. should be eulers number to the power of epsilon.

                Each query has a sensitivity attribute.
                
                A query could be "return the number of people that live in X city."
                This query could return response R.
                If you remove me from the dataset and I live in X city, the query would return a value R - 1 with 100% probability.
                To maintain privacy, we usually inject noise into the query response.
                Therefore, the removal of an entry would alter the discrete response probability distribution.
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://desfontain.es/blog/k-anonymity.html">k-anonymity</a>
            </td>
            <td>‚úÖ [completed]</td>
            <td>May 18, 2025</td>
            <td>
                This blog post explains how anonymized datasets can be joined with another dataset to de-anonymize a record. 
                The article states the definition of k-anonymity.
                The property requires that for any set of quasi-identifiers found in the original dataset at least k records contain that tuple of quasi-identifier values.
                You can use generalization and supression to achieve k-anonymity.
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://www.alexbowe.com/wavelet-trees/">Wavelet Trees: an Introduction</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://docs.google.com/document/d/e/2PACX-1vSqWbHFsAs7isXnoKVhUUQ53tpiVDgiEOV744PtB9TJ5-sk25mGUIQZNUbTROLaOGZpzotpVjO7SBoK/pub">Neuron Framework & Inference Team Info for External Candidates (Public Version)</a>
            </td>
            <td>üìù [reference]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://www.x402.org/x402-whitepaper.pdf">x402: An open standard for internet-native payments</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://docs.kernel.org/arch/x86/boot.html">The Linux/x86 Boot Protocol¬∂</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-1/">Writing a Debugger From Scratch - DbgRs Part 1 - Attaching to a Process</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">Method of Moments</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://tensara.org/problems">Tensara</a>
            </td>
            <td>üìù [problem set for cuda]</td>
            <td></td>
            <td></td>
        </tr>
    </table>

</body>

</html>
