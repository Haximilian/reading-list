<!DOCTYPE html>

<html lang="en">

<head>

    <meta charset="UTF-8" />

    <title>reading-list</title>

    <meta name="viewport" content="width=device-width,initial-scale=1" />

    <meta name="description" content="" />

    <link rel="icon" href="favicon.png">

</head>

<body>

    <h1>reading-list</h1>

    <table border="1" cellspacing="0" cellpadding="5">
        <tr>
            <th>title</th>
            <th>status</th>
            <th>date</th>
            <th>notes</th>
        </tr>
        <tr>
            <td>
                <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model</a>
            </td>
            <td>‚úÖ [completed]</td>
            <td>May 9, 2025</td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>
            </td>
            <td>‚úÖ [completed]</td>
            <td>May 11, 2025</td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a>
            </td>
            <td>‚úÖ [completed]</td>
            <td>May 23, 2025</td>
            <td>
                In word2vec, you learn embedding values such that the dot product of those embeddings corresponds to the likeliness of those words being neighbours. 
                The window size to determine wether a word‚Äôs a neighbour with another word is hyper-parameter. 
                The paper also uses ‚ÄúNoise Contrastive Estimation‚Äù to create negative examples. 
                Otherwise, the training dataset would only contain positive example. 
                The article didn‚Äôt state this explicitly but perceptrons essentially chain logistic regression; 
                each ‚Äúneuron‚Äù or node in a perceptron corresponds to a logistic regression which equals sigmoid(Bx). 
                If the first hidden layer contains 3 nodes, we can represent that activations of that layer as <LogisticRegression(B0, x), LogisticRegression(B1, x), LogisticRegression(B2, x)> where x represents the input to the network.
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>
            </td>
            <td>‚è≥ [partial]</td>
            <td>May 12, 2025; May 13 2025</td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://colah.github.io/posts/2015-09-Visual-Information/">Visual Information Theory</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43442.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>
            </td>
            <td>‚è≥ [partial]</td>
            <td>May 12, 2025</td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://arxiv.org/pdf/1607.06450">Layer Normalization</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://eoinmurray.info/boltzmann-machine">Boltzmann Machines</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://www.nist.gov/blogs/cybersecurity-insights/differential-privacy-privacy-preserving-data-analysis-introduction-our">Differential Privacy for Privacy-Preserving Data Analysis: An Introduction to our Blog Series</a>
            </td>
            <td>‚úÖ [completed]</td>
            <td>May 21, 2025</td>
            <td>
                The article states the definition of Differential Privacy.
                Given a dataset A and query Q, you should generate response R with P.a. probability.
                If you're given a dataset B such that B differs with A in only 1 records, for query Q,
                you should generate response R with P.b. probability.

                The ratio of P.a. and P.b. should be eulers number to the power of epsilon.

                Each query has a sensitivity attribute.
                
                A query could be "return the number of people that live in X city."
                This query could return response R.
                If you remove me from the dataset and I live in X city, the query would return a value R - 1 with 100% probability.
                To maintain privacy, we usually inject noise into the query response.
                Therefore, the removal of an entry would alter the discrete response probability distribution.
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://desfontain.es/blog/k-anonymity.html">k-anonymity</a>
            </td>
            <td>‚úÖ [completed]</td>
            <td>May 18, 2025</td>
            <td>
                This blog post explains how anonymized datasets can be joined with another dataset to de-anonymize a record. 
                The article states the definition of k-anonymity.
                The property requires that for any set of quasi-identifiers found in the original dataset at least k records contain that tuple of quasi-identifier values.
                You can use generalization and supression to achieve k-anonymity.
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://www.alexbowe.com/wavelet-trees/">Wavelet Trees: an Introduction</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://docs.google.com/document/d/e/2PACX-1vSqWbHFsAs7isXnoKVhUUQ53tpiVDgiEOV744PtB9TJ5-sk25mGUIQZNUbTROLaOGZpzotpVjO7SBoK/pub">Neuron Framework & Inference Team Info for External Candidates (Public Version)</a>
            </td>
            <td>üìù [reference]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://www.x402.org/x402-whitepaper.pdf">x402: An open standard for internet-native payments</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://docs.kernel.org/arch/x86/boot.html">The Linux/x86 Boot Protocol¬∂</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-1/">Writing a Debugger From Scratch - DbgRs Part 1 - Attaching to a Process</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">Method of Moments</a>
            </td>
            <td>‚ùå [todo]</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>
                <a href="https://tensara.org/problems">Tensara</a>
            </td>
            <td>üìù [problem set for cuda]</td>
            <td></td>
            <td></td>
        </tr>
    </table>

</body>

</html>
